
# Transformer

# Bert
## 为什么BERT在第一句前加一个[CLS]标志？
BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。

为什么选它呢，因为与文本中已有的其它词相比，这个无明显语义信息的符号会**更“公平”地融合文本中各个词的语义信息**，从而更好的表示整句话的语义。

具体来说，self-attention是用文本中其他词来增强目标词的语义表示，但目标词本身语义会占主要部分，因此，经过BERT的12层，每次词的embedding融合了所有词的信息，可以更好的表示自己的语义。

而[CLS]位本身没有语义，经过12层，得到的是attention后所有词的加权平均，相比其他正常词，可以更好表征句子语义。

# XLNet

# ALBERT

